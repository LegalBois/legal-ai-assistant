{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "from typing import List, Literal, Optional, Union\n",
    "\n",
    "import chromadb\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnableLambda\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langsmith import traceable\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/deniskirbaba/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # For MISTRAL_API_KEY and LangSmith tracing\n",
    "nltk.download(\"punkt_tab\")  # Tokenizer for BM-25 retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"llm\": {\"model_name\": \"mistral-large-latest\", \"temperature\": 0.7, \"top_p\": 0.9},\n",
    "    \"emb_model\": {\"model_name\": \"deepvk/USER-bge-m3\"},\n",
    "    \"query\": {\n",
    "        \"n_rephrase\": 2,\n",
    "        \"n_hyde\": 2,\n",
    "    },\n",
    "    \"retriever\": {\"k_simil\": 10, \"k_mmr\": 10, \"k_bm25\": 10, \"k_codex\": 10},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "llm = ChatMistralAI(**settings[\"llm\"])\n",
    "emb_model = HuggingFaceEmbeddings(**settings[\"emb_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query classification and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/system_prompt_1.txt\") as f:\n",
    "    system_prompt_1 = f.read()\n",
    "\n",
    "with open(\"prompts/examples_1.json\") as f:\n",
    "    examples_1 = json.load(f)\n",
    "\n",
    "example_prompt_1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"assistant\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt_1 = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt_1,\n",
    "    examples=examples_1,\n",
    ")\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_1),\n",
    "        few_shot_prompt_1,\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGQueries(BaseModel):\n",
    "    \"\"\"\n",
    "    Данные, сформированные на основе запроса пользователя для поиска релевантных документов по юридической практике в векторной БД\n",
    "    Необходимы для уточнения запроса и повышения релевантности выдачи\n",
    "    \"\"\"\n",
    "\n",
    "    keyinfo: str = Field(description=\"Ключевая информация\")\n",
    "    rephrase: List[str] = Field(\n",
    "        ...,\n",
    "        description=f\"{settings[\"query\"][\"n_rephrase\"]} различных запросов в векторную базу данных, содержащую судебные дела\",\n",
    "    )\n",
    "    hyde: List[str] = Field(\n",
    "        ...,\n",
    "        description=f\"{settings[\"query\"][\"n_hyde\"]} различных описаний реалистичных документа юридической практики\",\n",
    "    )\n",
    "    codex: Optional[\n",
    "        Literal[\"Гражданский кодекс\", \"Уголовный кодекс\", \"Административный кодекс\"]\n",
    "    ] = Field(\n",
    "        default=None,\n",
    "        description=\"Фильтр для уточнения правового поля запроса, используемый при обращении к векторной базе данных, если возможно определить\",\n",
    "    )\n",
    "\n",
    "\n",
    "class ChitChatResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Ответ на запрос пользователя, который не предполагает юридической консультации.\n",
    "    \"\"\"\n",
    "\n",
    "    response: str = Field(..., description=\"Ответ на общий или неюридический запрос пользователя.\")\n",
    "\n",
    "\n",
    "class FirstResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Итоговый результат обработки запроса пользователя.\n",
    "    В зависимости от характера запроса результат может быть представлен в двух форматах:\n",
    "    - RAGQueries: используется для юридических запросов. Содержит структурированные данные для обращения к векторной базе с целью поиска релевантных документов.\n",
    "    - ChitChatResponse: применяется для общих или неюридических запросов. Содержит текстовый ответ на запрос пользователя.\n",
    "    \"\"\"\n",
    "\n",
    "    response: Union[RAGQueries, ChitChatResponse] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Итоговый результат обработки запроса пользователя. \n",
    "    Форматы ответа:\n",
    "    - RAGQueries: для юридических запросов, содержит структурированные данные для поиска документов в векторной базе.\n",
    "    - ChitChatResponse: для общих запросов, содержит текстовый ответ.\"\"\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_1 = llm.with_structured_output(FirstResponse)\n",
    "chain_1 = prompt_1 | llm_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval and final generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def create_queries(inputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Creates queries for retriever:\n",
    "        1. Original user query\n",
    "        2. n_rephrase queries with key info + rephrase\n",
    "        3. n_hyde queries with key info + rephrase\n",
    "\n",
    "    Params:\n",
    "        `inputs` - output from previous LangChain Runnable in chain\n",
    "    \"\"\"\n",
    "    original_query: str = inputs[\"question\"]\n",
    "    llm_processed: RAGQueries = inputs[\"response_1\"].response\n",
    "\n",
    "    queries = [original_query]\n",
    "    for s in chain(llm_processed.rephrase, llm_processed.hyde):\n",
    "        queries.append(llm_processed.keyinfo + \"\\n\" + s)\n",
    "    return {\"queries\": queries, \"codex_filter\": llm_processed.codex}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ChromaDB with legal practices docs\n",
    "# Note: this DB should already be initialized\n",
    "persistent_client = chromadb.PersistentClient(path=\"../legal_practice_db/vector_storage\")\n",
    "legal_practices_store = Chroma(\n",
    "    client=persistent_client,\n",
    "    collection_name=\"legal_practices\",\n",
    "    embedding_function=emb_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retrievers(legal_practices_store: Chroma, retriever_settings: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Creates 4 retrievers, which will be ensembled later.\n",
    "\n",
    "    Params:\n",
    "        `legal_practices_store` - ChromaDB object of legal practices data\n",
    "        `retriever_settings` - settings for retrievers\n",
    "    \"\"\"\n",
    "    retrievers = {}\n",
    "    retrievers[\"simil\"] = legal_practices_store.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": retriever_settings[\"k_simil\"],\n",
    "        },\n",
    "    )\n",
    "\n",
    "    retrievers[\"mmr\"] = legal_practices_store.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": retriever_settings[\"k_mmr\"],\n",
    "            \"fetch_k\": 25,\n",
    "            \"lambda_mult\": 0.5,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    data_for_bm25 = legal_practices_store.get(include=[\"documents\", \"metadatas\"])\n",
    "    retrievers[\"bm25\"] = BM25Retriever.from_texts(\n",
    "        texts=data_for_bm25[\"documents\"],\n",
    "        metadatas=data_for_bm25[\"metadatas\"],\n",
    "        preprocess_func=word_tokenize,\n",
    "        k=retriever_settings[\"k_bm25\"],\n",
    "    )\n",
    "\n",
    "    retrievers[\"codex_filter\"] = legal_practices_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": retriever_settings[\"k_codex\"],\n",
    "            \"score_threshold\": 0.5,\n",
    "            # add at runtime: \"filter\": {\"codex\": {\"$eq\": \"?\"}},\n",
    "        },\n",
    "    )\n",
    "    return retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_practices_retrievers = create_retrievers(\n",
    "    legal_practices_store=legal_practices_store, retriever_settings=settings[\"retriever\"]\n",
    ")\n",
    "\n",
    "legal_practices_ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=list(legal_practices_retrievers.values()),\n",
    "    tags=list(legal_practices_retrievers.keys()),\n",
    "    weights=[0.25, 0.25, 0.25, 0.25],\n",
    "    c=60,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def batch_query(inputs) -> dict:\n",
    "    \"\"\"\n",
    "    Makes batch invoke of legal_practices_ensemble_retriever for each query.\n",
    "    Also set the filter for codex (if it's not None).\n",
    "\n",
    "    Params:\n",
    "        `inputs` - output from previous LangChain Runnable in chain\n",
    "    \"\"\"\n",
    "    # Add/remove codex filter for retriever\n",
    "    codex_poss_values = [\"А\", \"АГ\", \"АУ\", \"АГУ\", \"Г\", \"ГУ\", \"У\"]\n",
    "    if inputs[\"codex_filter\"] and inputs[\"codex_filter\"][0] in \"АГУ\":\n",
    "        legal_practices_ensemble_retriever.retrievers[3].search_kwargs[\"filter\"] = {\n",
    "            \"codex\": {\"$in\": [val for val in codex_poss_values if inputs[\"codex_filter\"][0] in val]}\n",
    "        }\n",
    "    else:\n",
    "        legal_practices_ensemble_retriever.retrievers[3].search_kwargs.pop(\"filter\", None)\n",
    "\n",
    "    # Batch retrieve\n",
    "    relevant_docs = legal_practices_ensemble_retriever.batch(inputs[\"queries\"])\n",
    "\n",
    "    return {\"relevant_docs\": relevant_docs, \"orig_query\": inputs[\"queries\"][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def prepare_docs(inputs) -> dict:\n",
    "    \"\"\"\n",
    "    Post-processing of retrieved relevant docs.\n",
    "    Steps:\n",
    "        1. Picks up the most relevant docs for each query w/o duplicates\n",
    "        2. Add `theme` from metadata of doc to its content\n",
    "        3. Merges all docs in one string\n",
    "\n",
    "    Params:\n",
    "        `inputs` - output from previous LangChain Runnable in chain\n",
    "    \"\"\"\n",
    "    relevant_docs = inputs[\"relevant_docs\"]\n",
    "    # Pick top document for each query without duplicates\n",
    "    seen_uids = set()\n",
    "    top_docs = []\n",
    "    for query_docs in relevant_docs:  # relevant_docs is a list of lists (docs per query)\n",
    "        for doc in query_docs:\n",
    "            uid = doc.metadata.get(\"uid\")\n",
    "            if uid not in seen_uids:\n",
    "                seen_uids.add(uid)\n",
    "                top_docs.append(doc)\n",
    "                break\n",
    "\n",
    "    # Merge `theme` into `page_content`\n",
    "    for doc in top_docs:\n",
    "        theme = doc.metadata.get(\"theme\", \"\")\n",
    "        doc.page_content = f\"{theme}\\n{doc.page_content}\"\n",
    "\n",
    "    # Merge all docs\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in top_docs)\n",
    "\n",
    "    return {\"context\": context, \"orig_query\": inputs[\"orig_query\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/system_prompt_2.txt\") as f:\n",
    "    system_prompt_2 = f.read()\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_2),\n",
    "        (\"user\", \"{orig_query}\\n===\\n{context}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseWithLegalDocs(BaseModel):\n",
    "    \"\"\"\n",
    "    Модель для структурированного ответа с юридическими документами.\n",
    "    \"\"\"\n",
    "\n",
    "    text_response: str = Field(\n",
    "        description=\"Полный текст юридического ответа на запрос пользователя.\"\n",
    "    )\n",
    "    legal_docs: List[str] = Field(\n",
    "        description=\"Список названий юридических документов, использованных для формирования ответа.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_2 = llm.with_structured_output(ResponseWithLegalDocs)\n",
    "\n",
    "chain_2 = (\n",
    "    RunnableLambda(create_queries)\n",
    "    | RunnableLambda(batch_query)\n",
    "    | RunnableLambda(prepare_docs)\n",
    "    | prompt_2\n",
    "    | llm_2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable\n",
    "def route(inputs) -> str | Runnable:\n",
    "    \"\"\"\n",
    "    Swithes behavior of pipeline based on query classification:\n",
    "        1. If LLM classify query as chit-chat - we just return the text response without doing RAG\n",
    "        2. Otherwise, if query belongs to legal field - execute `chain_2` for RAG\n",
    "\n",
    "    Params:\n",
    "        `inputs` - output from previous LangChain Runnable in chain\n",
    "    \"\"\"\n",
    "    if isinstance(inputs[\"response_1\"].response, ChitChatResponse):\n",
    "        return inputs[\"response_1\"].response.response\n",
    "    else:\n",
    "        return chain_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chain = {\"response_1\": chain_1, \"question\": itemgetter(\"question\")} | RunnableLambda(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_core.runnables.base.RunnableSequence'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_core.runnables.base.RunnableLambda'>)\n"
     ]
    }
   ],
   "source": [
    "query = \"Я ехал на велосипеде, упал и поцарапал машину, что делать и что мне будет?\"\n",
    "response = final_chain.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Ваш случай относится к дорожно-транспортному происшествию (ДТП), в котором вы, будучи велосипедистом, повредили автомобиль. Важно понимать, что в таких ситуациях применяются нормы Кодекса Российской Федерации об административных правонарушениях (КоАП РФ) и Правил дорожного движения Российской Федерации (ПДД РФ).\n",
       "\n",
       "1. **Обязанности участников ДТП**: Согласно пункту 2.5 ПДД РФ, водитель транспортного средства обязан соблюдать осторожность и быть внимательным к другим участникам движения, включая велосипедистов. В вашем случае, если вы не соблюдали правила движения, это может быть расценено как административное правонарушение.\n",
       "\n",
       "2. **Ответственность за причинение вреда**: В соответствии с частью 1 статьи 12.24 КоАП РФ, нарушение ПДД, повлекшее причинение легкого или средней тяжести вреда здоровью, влечет административную ответственность. В вашем случае, если вы повредили автомобиль, это может быть квалифицировано как нарушение, повлекшее материальный ущерб.\n",
       "\n",
       "3. **Действия после ДТП**: Вам необходимо остановиться, включить аварийную сигнализацию и выставить знак аварийной остановки. Также обязательно сообщить о ДТП в полицию и дождаться прибытия сотрудников. Важно зафиксировать все обстоятельства происшествия, включая фотографии и контактные данные свидетелей.\n",
       "\n",
       "4. **Возмещение ущерба**: Владелец поврежденного автомобиля имеет право требовать от вас возмещения ущерба. Это может быть сделано через страховую компанию, если у вас есть полис ОСАГО, или через суд, если страхования нет.\n",
       "\n",
       "**Рекомендации**:\n",
       "- Оставайтесь на месте происшествия и вызовите полицию.\n",
       "- Зафиксируйте все обстоятельства ДТП.\n",
       "- Обратитесь в свою страховую компанию для урегулирования вопроса о возмещении ущерба.\n",
       "- Если у вас нет страховки, будьте готовы к возмещению ущерба в судебном порядке.\n",
       "\n",
       "**Использованные документы**:\n",
       "- Кодекс Российской Федерации об административных правонарушениях (КоАП РФ)\n",
       "- Правила дорожного движения Российской Федерации (ПДД РФ)\n",
       "- Статья 12.24 КоАП РФ\n",
       "- Пункт 2.5 ПДД РФ"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Кодекс Российской Федерации об административных правонарушениях (КоАП РФ)',\n",
       " 'Правила дорожного движения Российской Федерации (ПДД РФ)',\n",
       " 'Статья 12.24 КоАП РФ',\n",
       " 'Пункт 2.5 ПДД РФ']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.legal_docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicing-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
